{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Access Google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcrBmwi8X_bs",
        "outputId": "5c829484-4106-45e4-c370-0e5ecea0a55a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "\n",
        "# Load datasets\n",
        "def load_datasets():\n",
        "    ratingDataset = pd.read_csv('/content/drive/MyDrive/42913 SIN/ua.base', sep='\\t', header=None, names=['user id', 'item id', 'rating', 'timestamp'])\n",
        "    userDataset = pd.read_csv('/content/drive/MyDrive/42913 SIN/u.user', sep='|', header=None, names=['user id', 'age', 'gender', 'occupation', 'zip code'])\n",
        "    movieDataset = pd.read_csv('/content/drive/MyDrive/42913 SIN/u.item', sep='|', header=None, encoding='latin1', names=['movie id', 'movie title', 'release date', 'video release date', 'IMDb URL', 'unknown cat', 'Action', 'Adventure', 'Animation', ' Childrens', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical','Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western'])\n",
        "    return ratingDataset, userDataset, movieDataset\n",
        "\n",
        "# Check missing values\n",
        "def check_missing_values(datasets):\n",
        "    ratingDataset, userDataset, movieDataset = datasets\n",
        "    print(ratingDataset.isnull().sum())\n",
        "    print('---------------------------')\n",
        "    print(userDataset.isnull().sum())\n",
        "    print('---------------------------')\n",
        "    print(movieDataset.isnull().sum())\n",
        "    print('---------------------------')\n",
        "\n",
        "# Perform Singular Value Decomposition (SVD)\n",
        "def perform_svd(user_item_matrix):\n",
        "    U, Sigma, Vt = np.linalg.svd(user_item_matrix, full_matrices=False)\n",
        "    return U, Sigma, Vt\n",
        "\n",
        "# Collaborative Filtering - Predict rating\n",
        "def predict_rating(user_id, movie_id, user_item_matrix, user_similarities,k=10):\n",
        "    similar_users_ratings = user_item_matrix.loc[:, movie_id]\n",
        "    similar_users = similar_users_ratings.sort_values(ascending=False).index[:k]\n",
        "    weighted_sum = 0\n",
        "    total_similarity = 0\n",
        "    for similar_user_id in similar_users:\n",
        "        similarity = user_similarities[user_id][similar_user_id]\n",
        "        rating = user_item_matrix.loc[similar_user_id, movie_id]\n",
        "        weighted_sum += similarity * rating\n",
        "        total_similarity += similarity\n",
        "    if total_similarity == 0:\n",
        "        return 0\n",
        "    predicted_rating = weighted_sum / total_similarity\n",
        "    return predicted_rating\n",
        "\n",
        "\n",
        "# Collaborative Filtering - Recommend movies\n",
        "def recommend_movies_CF(user_id, user_item_matrix, user_similarities, movie_id_to_name, num_recommendations=5):\n",
        "    similar_users = sorted(list(enumerate(user_similarities[user_id])), key=lambda x: x[1], reverse=True)\n",
        "    rated_movies = set(user_item_matrix.loc[user_id][user_item_matrix.loc[user_id] > 0].index)\n",
        "    recommended_movies = []\n",
        "    for user, similarity in similar_users:\n",
        "        if len(recommended_movies) >= num_recommendations:\n",
        "            break\n",
        "        if user != user_id:\n",
        "            movies_rated_by_similar_user = user_item_matrix.loc[user][user_item_matrix.loc[user] > 0].index\n",
        "            for movie in movies_rated_by_similar_user:\n",
        "                if movie not in rated_movies:\n",
        "                    predicted_rating = predict_rating(user_id, movie, user_item_matrix, user_similarities)\n",
        "                    recommended_movies.append((movie, predicted_rating))\n",
        "                    if len(recommended_movies) >= num_recommendations:\n",
        "                        break\n",
        "    return recommended_movies\n",
        "\n",
        "\n",
        "# Content-Based Filtering - Preprocess movie features\n",
        "def preprocess_movie_features(movieDataset):\n",
        "    movie_descriptions = [' '.join(map(str, row[1:])) for _, row in movieDataset.iterrows()]\n",
        "    return movie_descriptions\n",
        "\n",
        "# Content-Based Filtering - Recommend movies\n",
        "def recommend_movies_CB(movie_title, cosine_sim_matrix, movieDataset, movie_id_to_name, top_n=5):\n",
        "    idx = movieDataset[movieDataset['movie title'] == movie_title].index.values[0]\n",
        "    sim_scores = list(enumerate(cosine_sim_matrix[idx]))\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "    sim_scores = sim_scores[1:top_n + 1]\n",
        "    recommended_movies = [(movie_id_to_name[movieDataset.iloc[idx]['movie id']], sim_score) for idx, sim_score in sim_scores]\n",
        "    return recommended_movies\n",
        "\n",
        "# Define RNN model architecture\n",
        "def build_rnn_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(64, input_shape=input_shape))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "    return model\n",
        "\n",
        "# Map movie ID to movie name\n",
        "def map_movie_id_to_name(movieDataset):\n",
        "    movie_id_to_name = {}\n",
        "    for index, row in movieDataset.iterrows():\n",
        "        movie_id = row['movie id']\n",
        "        movie_title = row['movie title']\n",
        "        movie_id_to_name[movie_id] = movie_title\n",
        "    return movie_id_to_name\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "def split_data(user_item_matrix):\n",
        "    X = user_item_matrix.iloc[:, :-1].values  # Features (user-item interactions)\n",
        "    y = user_item_matrix.iloc[:, -1].values   # Target ratings\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# Function to evaluate Collaborative Filtering\n",
        "def evaluate_collaborative_filtering(actual_ratings, predicted_ratings, threshold=3.5):\n",
        "    # Calculate RMSE\n",
        "    rmse = mean_squared_error(actual_ratings, predicted_ratings, squared=False)\n",
        "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
        "\n",
        "    # Calculate MAE\n",
        "    mae = mean_absolute_error(actual_ratings, predicted_ratings)\n",
        "    print(\"Mean Absolute Error (MAE):\", mae)\n",
        "\n",
        "    # Convert ratings to binary for Precision and Recall\n",
        "    actual_binary = (actual_ratings > threshold).astype(int)\n",
        "    predicted_binary = (predicted_ratings > threshold).astype(int)\n",
        "\n",
        "    # Calculate Precision\n",
        "    precision = precision_score(actual_binary, predicted_binary)\n",
        "    print(\"Precision:\", precision)\n",
        "\n",
        "    # Calculate Recall\n",
        "    recall = recall_score(actual_binary, predicted_binary)\n",
        "    print(\"Recall:\", recall)\n",
        "\n",
        "    # Calculate F1 Score\n",
        "    f1 = f1_score(actual_binary, predicted_binary)\n",
        "    print(\"F1 Score:\", f1)\n",
        "\n",
        "    # Calculate Hit Rate\n",
        "    hit_rate = sum((actual_binary == 1) & (predicted_binary == 1)) / sum(actual_binary == 1)\n",
        "    print(\"Hit Rate:\", hit_rate)\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    ratingDataset, userDataset, movieDataset = load_datasets()\n",
        "    #check_missing_values((ratingDataset, userDataset, movieDataset))\n",
        "\n",
        "    # Collaborative Filtering\n",
        "    k = 10\n",
        "    user_similarities = cosine_similarity(user_item_matrix)\n",
        "    movie_id_to_name = map_movie_id_to_name(movieDataset)\n",
        "    user_id = 1\n",
        "    recommended_movies_CF = recommend_movies_CF(user_id, user_item_matrix, user_similarities, movie_id_to_name)\n",
        "    print(\"Recommended movies using Collaborative Filtering for User\", user_id, \":\")\n",
        "    for movie_id, predicted_rating in recommended_movies_CF:\n",
        "        movie_title = movie_id_to_name.get(movie_id, \"Unknown\")\n",
        "        print(movie_title, \"-\", \"Predicted Rating:\", predicted_rating)\n",
        "    # Evaluate Collaborative Filtering\n",
        "    #evaluate_collaborative_filtering(y_test, y_pred)\n",
        "\n",
        "    # Content-Based Filtering\n",
        "    movie_descriptions = preprocess_movie_features(movieDataset)\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(movie_descriptions)\n",
        "    cosine_sim_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "    recommended_movies_CB = recommend_movies_CB('Toy Story (1995)', cosine_sim_matrix, movieDataset, movie_id_to_name)\n",
        "    print(\"\\nRecommended movies using Content-Based Filtering for 'Toy Story (1995)':\")\n",
        "    for movie_title, similarity_score in recommended_movies_CB:\n",
        "        print(movie_title, \"-\", \"Similarity Score:\", similarity_score)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AL-gDLLNV3hd",
        "outputId": "dbe28cf1-558c-4f60-98be-820076638a9a"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recommended movies using Collaborative Filtering for User 1 :\n",
            "Rock, The (1996) - Predicted Rating: 5.0\n",
            "Delicatessen (1991) - Predicted Rating: 5.000000000000001\n",
            "Hunt for Red October, The (1990) - Predicted Rating: 5.0\n",
            "Sabrina (1995) - Predicted Rating: 5.0\n",
            "Sense and Sensibility (1995) - Predicted Rating: 4.999999999999999\n",
            "\n",
            "Recommended movies using Content-Based Filtering for 'Toy Story (1995)':\n",
            "Pyromaniac's Love Story, A (1995) - Similarity Score: 0.36556028709520605\n",
            "Fear, The (1995) - Similarity Score: 0.2510364915084959\n",
            "My Family (1995) - Similarity Score: 0.24019518012909064\n",
            "Show, The (1995) - Similarity Score: 0.234401494688814\n",
            "Sabrina (1995) - Similarity Score: 0.232498494224088\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ratingDataset, userDataset, movieDataset = load_datasets()\n",
        "check_missing_values((ratingDataset, userDataset, movieDataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKSLSuh5314G",
        "outputId": "1e1ccb51-0f61-48eb-974d-0549151f1e6f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user id      0\n",
            "item id      0\n",
            "rating       0\n",
            "timestamp    0\n",
            "dtype: int64\n",
            "---------------------------\n",
            "user id       0\n",
            "age           0\n",
            "gender        0\n",
            "occupation    0\n",
            "zip code      0\n",
            "dtype: int64\n",
            "---------------------------\n",
            "movie id                 0\n",
            "movie title              0\n",
            "release date             1\n",
            "video release date    1682\n",
            "IMDb URL                 3\n",
            "unknown cat              0\n",
            "Action                   0\n",
            "Adventure                0\n",
            "Animation                0\n",
            " Childrens               0\n",
            "Comedy                   0\n",
            "Crime                    0\n",
            "Documentary              0\n",
            "Drama                    0\n",
            "Fantasy                  0\n",
            "Film-Noir                0\n",
            "Horror                   0\n",
            "Musical                  0\n",
            "Mystery                  0\n",
            "Romance                  0\n",
            "Sci-Fi                   0\n",
            "Thriller                 0\n",
            "War                      0\n",
            "Western                  0\n",
            "dtype: int64\n",
            "---------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SVD\n",
        "merged_data = pd.merge(ratingDataset, userDataset, on='user id')\n",
        "merged_data = pd.merge(merged_data, movieDataset, left_on='item id', right_on='movie id')\n",
        "user_item_matrix = pd.pivot_table(merged_data, values='rating', index='user id', columns='item id', fill_value=0)\n",
        "U, Sigma, Vt = perform_svd(user_item_matrix)\n",
        "\n",
        "# Convert user_item_matrix DataFrame to NumPy array\n",
        "user_item_array = user_item_matrix.values\n",
        "\n",
        "# Get the indices of non-zero ratings\n",
        "nonzero_indices = np.argwhere(user_item_array > 0)\n",
        "\n",
        "# Split the indices into training and testing sets\n",
        "train_indices, test_indices = train_test_split(nonzero_indices, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create training and testing data arrays\n",
        "X_train = train_indices[:, 0], train_indices[:, 1]  # user ID, item ID\n",
        "y_train = user_item_array[train_indices[:, 0], train_indices[:, 1]]  # ratings\n",
        "\n",
        "X_test = test_indices[:, 0], test_indices[:, 1]  # user ID, item ID\n",
        "y_test = user_item_array[test_indices[:, 0], test_indices[:, 1]]  # ratings\n",
        "\n",
        "# Split data into train and test sets\n",
        "#X_train, X_test, y_train, y_test = split_data(user_item_matrix)\n",
        "\n",
        "# Reshape input data for LSTM\n",
        "#X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "#X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))"
      ],
      "metadata": {
        "id": "e4ItdCZA3ban"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"RNN:\")\n",
        "\n",
        "# Build and train the RNN model\n",
        "model = build_rnn_model(input_shape=(X_train.shape[1], 1))\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test))\n",
        "\n",
        "# Predict ratings using the trained RNN model\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# RNN metrics\n",
        "print(\"Evaluate RNN:\")\n",
        "\n",
        "# Calculate Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "\n",
        "# Calculate RMSE\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(\"Root Mean Squared Error (RMSE):\", mse)\n",
        "\n",
        "# Predicted ratings above a certain threshold are considered positive predictions\n",
        "threshold = 2.5\n",
        "positive_predictions = y_pred > threshold\n",
        "\n",
        "# Actual ratings above a certain threshold are considered relevant items\n",
        "relevant_items = y_test > threshold\n",
        "\n",
        "# Calculate Precision\n",
        "if np.any(positive_predictions):\n",
        "    precision = sum(positive_predictions & relevant_items) / sum(positive_predictions)\n",
        "else:\n",
        "    precision = 0.0\n",
        "print(\"Precision:\", precision)\n",
        "\n",
        "# Calculate Recall\n",
        "if np.any(relevant_items):\n",
        "    recall = sum(positive_predictions & relevant_items) / sum(relevant_items)\n",
        "else:\n",
        "    recall = 0.0\n",
        "print(\"Recall:\", recall)\n",
        "\n",
        "# Calculate F1 Score\n",
        "if np.any(relevant_items) and np.any(positive_predictions):\n",
        "  f1 = f1_score(relevant_items, positive_predictions)\n",
        "else:\n",
        "  f1 = 0.0\n",
        "print(\"F1:\", f1)\n",
        "\n",
        "# Calculate Hit Rate\n",
        "if np.any(relevant_items):\n",
        "    hit_rate = sum(positive_predictions & relevant_items) / sum(relevant_items)\n",
        "else:\n",
        "    hit_rate = 0.0\n",
        "print(\"Hit Rate:\", hit_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQ9pM89b3hmq",
        "outputId": "0fce858e-f06c-428f-c238-d5bb5beb96a4"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNN:\n",
            "Epoch 1/10\n",
            "12/12 [==============================] - 13s 812ms/step - loss: 0.0120 - val_loss: 1.1425e-05\n",
            "Epoch 2/10\n",
            "12/12 [==============================] - 11s 885ms/step - loss: 0.0120 - val_loss: 1.6282e-05\n",
            "Epoch 3/10\n",
            "12/12 [==============================] - 11s 921ms/step - loss: 0.0120 - val_loss: 7.7100e-05\n",
            "Epoch 4/10\n",
            "12/12 [==============================] - 11s 905ms/step - loss: 0.0120 - val_loss: 1.4200e-05\n",
            "Epoch 5/10\n",
            "12/12 [==============================] - 9s 772ms/step - loss: 0.0119 - val_loss: 1.0161e-04\n",
            "Epoch 6/10\n",
            "12/12 [==============================] - 11s 912ms/step - loss: 0.0119 - val_loss: 1.0055e-05\n",
            "Epoch 7/10\n",
            "12/12 [==============================] - 11s 918ms/step - loss: 0.0120 - val_loss: 8.6139e-06\n",
            "Epoch 8/10\n",
            "12/12 [==============================] - 11s 944ms/step - loss: 0.0119 - val_loss: 4.0876e-05\n",
            "Epoch 9/10\n",
            "12/12 [==============================] - 9s 765ms/step - loss: 0.0119 - val_loss: 2.2169e-05\n",
            "Epoch 10/10\n",
            "12/12 [==============================] - 11s 918ms/step - loss: 0.0119 - val_loss: 1.8007e-05\n",
            "6/6 [==============================] - 1s 172ms/step\n",
            "Evaluate RNN:\n",
            "Mean Squared Error (MSE): 1.800735051760878e-05\n",
            "Root Mean Squared Error (RMSE): 1.800735051760878e-05\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "F1: 0.0\n",
            "Hit Rate: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "# Implement NCF model\n",
        "def build_ncf_model(num_users, num_items, latent_dim=64):\n",
        "    # Input layers\n",
        "    user_input = tf.keras.layers.Input(shape=(1,))\n",
        "    item_input = tf.keras.layers.Input(shape=(1,))\n",
        "\n",
        "    # User embedding layer\n",
        "    user_embedding = tf.keras.layers.Embedding(input_dim=num_users, output_dim=latent_dim)(user_input)\n",
        "    user_embedding = tf.keras.layers.Flatten()(user_embedding)\n",
        "\n",
        "    # Item embedding layer\n",
        "    item_embedding = tf.keras.layers.Embedding(input_dim=num_items, output_dim=latent_dim)(item_input)\n",
        "    item_embedding = tf.keras.layers.Flatten()(item_embedding)\n",
        "\n",
        "    # Concatenate user and item embeddings\n",
        "    concat = tf.keras.layers.Concatenate()([user_embedding, item_embedding])\n",
        "\n",
        "    # Fully connected layers\n",
        "    dense1 = tf.keras.layers.Dense(64, activation='relu')(concat)\n",
        "    dense2 = tf.keras.layers.Dense(32, activation='relu')(dense1)\n",
        "\n",
        "    # Output layer\n",
        "    output = tf.keras.layers.Dense(1, activation='sigmoid')(dense2)\n",
        "\n",
        "    # Create model\n",
        "    model = tf.keras.Model(inputs=[user_input, item_input], outputs=output)\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Get number of unique users and items\n",
        "num_users = ratingDataset['user id'].nunique()\n",
        "num_items = ratingDataset['item id'].nunique()\n",
        "\n",
        "\n",
        "# Build NCF model\n",
        "ncf_model = build_ncf_model(num_users, num_items)\n",
        "\n",
        "# Train NCF model\n",
        "ncf_model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test))\n",
        "\n",
        "# Make predictions using NCF model\n",
        "predictions = ncf_model.predict(X_test)\n",
        "\n",
        "\n",
        "# Calculate evaluation metrics for NCF\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "precision = precision_score(y_test > 3, predictions > 0.5)\n",
        "recall = recall_score(y_test > 3, predictions > 0.5)\n",
        "f1 = f1_score(y_test > 3, predictions > 0.5)\n",
        "\n",
        "# Print evaluation metrics for NCF\n",
        "print(\"NCF Metrics:\")\n",
        "print(\"RMSE:\", np.sqrt(mse))\n",
        "print(\"MAE:\", mae)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtNF50xq5Dqc",
        "outputId": "5062c195-ffac-496c-c7fc-ce5e199a3906"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1133/1133 [==============================] - 9s 6ms/step - loss: -967371.3125 - accuracy: 0.0615 - val_loss: -4835621.5000 - val_accuracy: 0.0612\n",
            "Epoch 2/10\n",
            "1133/1133 [==============================] - 5s 5ms/step - loss: -25217050.0000 - accuracy: 0.0615 - val_loss: -60192928.0000 - val_accuracy: 0.0612\n",
            "Epoch 3/10\n",
            "1133/1133 [==============================] - 7s 6ms/step - loss: -134336096.0000 - accuracy: 0.0615 - val_loss: -235218752.0000 - val_accuracy: 0.0612\n",
            "Epoch 4/10\n",
            "1133/1133 [==============================] - 5s 5ms/step - loss: -395932192.0000 - accuracy: 0.0615 - val_loss: -595780928.0000 - val_accuracy: 0.0612\n",
            "Epoch 5/10\n",
            "1133/1133 [==============================] - 8s 7ms/step - loss: -875091520.0000 - accuracy: 0.0615 - val_loss: -1207647360.0000 - val_accuracy: 0.0612\n",
            "Epoch 6/10\n",
            "1133/1133 [==============================] - 6s 5ms/step - loss: -1639434240.0000 - accuracy: 0.0615 - val_loss: -2140856192.0000 - val_accuracy: 0.0612\n",
            "Epoch 7/10\n",
            "1133/1133 [==============================] - 5s 5ms/step - loss: -2761031680.0000 - accuracy: 0.0615 - val_loss: -3470226944.0000 - val_accuracy: 0.0612\n",
            "Epoch 8/10\n",
            "1133/1133 [==============================] - 6s 6ms/step - loss: -4318798848.0000 - accuracy: 0.0615 - val_loss: -5277495296.0000 - val_accuracy: 0.0612\n",
            "Epoch 9/10\n",
            "1133/1133 [==============================] - 5s 5ms/step - loss: -6396767744.0000 - accuracy: 0.0615 - val_loss: -7652181504.0000 - val_accuracy: 0.0612\n",
            "Epoch 10/10\n",
            "1133/1133 [==============================] - 6s 5ms/step - loss: -9085824000.0000 - accuracy: 0.0615 - val_loss: -10683181056.0000 - val_accuracy: 0.0612\n",
            "567/567 [==============================] - 1s 2ms/step\n",
            "NCF Metrics:\n",
            "RMSE: 2.7762418925665924\n",
            "MAE: 2.5368223473556366\n",
            "Precision: 0.5561444186816826\n",
            "Recall: 1.0\n",
            "F1 Score: 0.7147722435078756\n"
          ]
        }
      ]
    }
  ]
}